{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get dataset files\n",
    "\n",
    "cars_listing_1_dir = '../datasets/car-listing-1'\n",
    "cars_listing_2_dir = '../datasets/car-listing-2'\n",
    "\n",
    "def get_csvs_in_dir(dir_path):\n",
    "    return [join(dir_path, f) for f in listdir(dir_path) if isfile(join(dir_path, f)) and f.endswith('.csv')]\n",
    "\n",
    "\n",
    "cars_1_files = get_csvs_in_dir(cars_listing_1_dir)\n",
    "cars_2_files = get_csvs_in_dir(cars_listing_2_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# peek at cars 1 datasets\n",
    "df_audi = pd.read_csv(cars_1_files[0])\n",
    "print(df_audi.head())\n",
    "\n",
    "df_bmw = pd.read_csv(cars_1_files[1])\n",
    "print(df_bmw.head())\n",
    "\n",
    "df_ford = pd.read_csv(cars_1_files[2])\n",
    "print(df_ford.head())\n",
    "\n",
    "df_hyundi = pd.read_csv(cars_1_files[3])\n",
    "print(df_hyundi.head())\n",
    "\n",
    "df_merc = pd.read_csv(cars_1_files[4])\n",
    "print(df_merc.head())\n",
    "\n",
    "df_toyota = pd.read_csv(cars_1_files[5])\n",
    "print(df_toyota.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add column for brand of car\n",
    "df_audi['brand']   = 'audi'\n",
    "df_bmw['brand']    = 'bmw'\n",
    "df_ford['brand']   = 'ford'\n",
    "df_hyundi['brand'] = 'hyundi'\n",
    "df_merc['brand']   = 'merc'\n",
    "df_toyota['brand'] = 'toyota'\n",
    "\n",
    "# concatenate all dataframes together\n",
    "df_cars_1 = pd.concat([df_audi, df_bmw, df_ford, df_hyundi, df_merc, df_toyota])\n",
    "\n",
    "# change column order to something that allows us to split it easier later on\n",
    "df_cars_1 = df_cars_1[['brand', 'model', 'transmission', 'fuelType','year', 'mileage', 'tax', 'mpg', 'engineSize', 'price']]\n",
    "cars_1_y = df_cars_1.pop('price').to_numpy()\n",
    "cars_1_X = df_cars_1.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data\n",
    "Using OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temporarily separate categorical cols from numerical\n",
    "num_cols = cars_1_X[:,4:]\n",
    "cat_cols = cars_1_X[:, :4]\n",
    "\n",
    "# One-Hot Encode string values\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "cat_cols_enc = enc.fit_transform(cat_cols)\n",
    "\n",
    "cars_1_X_enc = np.hstack((cat_cols_enc, num_cols)).astype(np.float32)\n",
    "print(cars_1_X_enc.shape)\n",
    "print(cars_1_X_enc[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cars_1_X_enc, cars_1_y, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"X_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "## 1. MAE: \n",
    "- It is not very sensitive to outliers in comparison to MSE since it doesn't punish huge errors. \n",
    "- It is usually used when the performance is measured on continuous variable data. \n",
    "- It gives a linear value, which averages the weighted individual differences equally. \n",
    "- The lower the value, better is the model's performance.\n",
    "\n",
    "## 2. MSE: \n",
    "- It is one of the most commonly used metrics, but least useful when a single bad prediction \n",
    "- would ruin the entire model's predicting abilities, i.e when the dataset contains a lot of noise. \n",
    "- It is most useful when the dataset contains outliers, or unexpected values(too high or too low values).\n",
    "\n",
    "## 3. RMSE: \n",
    "- In RMSE, the errors are squared before they are averaged.\n",
    "- This basically implies that RMSE assigns a higher weight to larger errors. \n",
    "- This indicates that RMSE is much more useful when large errors are present and they drastically affect the model's performance. \n",
    "- It avoids taking the absolute value of the error and this trait is useful in many mathematical calculations.\n",
    "- In this metric also, the lower the value, better is the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# TODO - try other loss functions and determine best to use\n",
    "\n",
    "\n",
    "def BaselineModel(optimizer='adam', loss='mean_squared_error',\n",
    "                  activation='relu', output_activation='linear',\n",
    "                  kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                  input_neurons=150, hidden_neurons=200, num_hidden_layers=2):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define INPUT layer\n",
    "    model.add(Dense(input_neurons, input_dim=X_train.shape[1],\n",
    "                    activation=activation,\n",
    "                    kernel_initializer=kernel_initializer, \n",
    "                    bias_initializer=bias_initializer,\n",
    "                    name='layer_input'))\n",
    "    \n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(hidden_neurons, activation=activation,\n",
    "                        kernel_initializer=kernel_initializer, \n",
    "                        bias_initializer=bias_initializer,\n",
    "                        name=f'layer_hidden_{i}'))\n",
    "    \n",
    "    # Define OUTPUT layer\n",
    "    model.add(Dense(1, activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer,\n",
    "                    bias_initializer=bias_initializer, \n",
    "                    name='layer_output'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    # Print out model summary\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define checkpoint callback for model saving\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5'\n",
    "log_dir = 'tensorboard-logs' \n",
    "\n",
    "checkpoint = ModelCheckpoint(f'models/{checkpoint_name}', monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping, tensorboard]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Run with default parameters\n",
    "\n",
    "coarse_grid = [\n",
    "    {\n",
    "        'batch_size': [10, 50, 100, 200, 400],\n",
    "        'epochs': [25, 50, 100],\n",
    "        'optimizer': ['Adam'],\n",
    "        'activation': ['relu'],\n",
    "        'num_hidden_layers': [1, 2, 5],\n",
    "        'hidden_neurons': [50, 100, 200, 500]\n",
    "    }\n",
    "]\n",
    "\n",
    "cross_val = 5\n",
    "scores = ['precision', 'recall']\n",
    "result = {}\n",
    "for score in scores:\n",
    "    print('-' * 50)\n",
    "    print(\n",
    "        f\"# Tuning hyper-parameters for {score} with {cross_val}-fold cross-validation\")\n",
    "    print()\n",
    "\n",
    "    # Employ GridSearch using the cross_val variable on the param grid provided\n",
    "    clf = GridSearchCV(\n",
    "        KerasRegressor(build_fn=BaselineModel, batch_size=100, epochs=100, verbose=2),\n",
    "        coarse_grid,\n",
    "        scoring='%s_macro' % score,\n",
    "        # cv=cross_val,\n",
    "        n_jobs=1,\n",
    "        verbose=2\n",
    "    )\n",
    "    # Fit the model on the training labels and outputs\n",
    "    \n",
    "    clf.fit(X_train, y_train,\n",
    "            shuffle=True)\n",
    "    \n",
    "    # clf.fit(X_train, y_train,\n",
    "    #         callbacks=[early_stopping],\n",
    "    #         verbose=2, shuffle=True,\n",
    "    #         workers=2, use_multiprocessing=True)\n",
    "    \n",
    "\n",
    "    print(\"# Best parameters set found on development set:\")\n",
    "    print(f'\\t{clf.best_params_}')\n",
    "    print()\n",
    "    result[score] = clf.best_params_\n",
    "    print(\"# Grid scores on development set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"\\t - %0.3f (+/-%0.03f) for %r\"\n",
    "                % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"# Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# history = model_0.fit(X_train, y_train, validation_split=0.2,\n",
    "#                       batch_size=50, epochs=100,\n",
    "#                         callbacks=callbacks_list,\n",
    "#                         verbose=1, shuffle=True,\n",
    "#                         workers=8, use_multiprocessing=True)\n",
    "\n",
    "# Print the precision and recall of the model.\n",
    "# y_pred = model_0.predict(X_test)\n",
    "\n",
    "# print(\"MAE\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "# print(history[1].params)\n",
    "\n",
    "# loss_train = history['train_loss']\n",
    "# loss_val = history['val_loss']\n",
    "# epochs = range(1, 100)\n",
    "# plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "# plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "# plt.title('Training and Validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# loss_train = history['acc']\n",
    "# loss_val = history['val_acc']\n",
    "# epochs = range(1, 100)\n",
    "# plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
    "# plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
    "# plt.title('Training and Validation accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
