{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get dataset files\n",
    "\n",
    "cars_listing_1_dir = '../datasets/car-listing-1'\n",
    "cars_listing_2_dir = '../datasets/car-listing-2'\n",
    "\n",
    "def get_csvs_in_dir(dir_path):\n",
    "    return [join(dir_path, f) for f in listdir(dir_path) if isfile(join(dir_path, f)) and f.endswith('.csv')]\n",
    "\n",
    "\n",
    "cars_1_files = get_csvs_in_dir(cars_listing_1_dir)\n",
    "cars_2_files = get_csvs_in_dir(cars_listing_2_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model  year  price transmission  mileage fuelType  tax   mpg  engineSize\n",
      "0    A1  2017  12500       Manual    15735   Petrol  150  55.4         1.4\n",
      "1    A6  2016  16500    Automatic    36203   Diesel   20  64.2         2.0\n",
      "2    A1  2016  11000       Manual    29946   Petrol   30  55.4         1.4\n",
      "3    A4  2017  16800    Automatic    25952   Diesel  145  67.3         2.0\n",
      "4    A3  2019  17300       Manual     1998   Petrol  145  49.6         1.0\n",
      "       model  year  price transmission  mileage fuelType  tax   mpg  \\\n",
      "0   5 Series  2014  11200    Automatic    67068   Diesel  125  57.6   \n",
      "1   6 Series  2018  27000    Automatic    14827   Petrol  145  42.8   \n",
      "2   5 Series  2016  16000    Automatic    62794   Diesel  160  51.4   \n",
      "3   1 Series  2017  12750    Automatic    26676   Diesel  145  72.4   \n",
      "4   7 Series  2014  14500    Automatic    39554   Diesel  160  50.4   \n",
      "\n",
      "   engineSize  \n",
      "0         2.0  \n",
      "1         2.0  \n",
      "2         3.0  \n",
      "3         1.5  \n",
      "4         3.0  \n",
      "     model  year  price transmission  mileage fuelType  tax   mpg  engineSize\n",
      "0   Fiesta  2017  12000    Automatic    15944   Petrol  150  57.7         1.0\n",
      "1    Focus  2018  14000       Manual     9083   Petrol  150  57.7         1.0\n",
      "2    Focus  2017  13000       Manual    12456   Petrol  150  57.7         1.0\n",
      "3   Fiesta  2019  17500       Manual    10460   Petrol  145  40.3         1.5\n",
      "4   Fiesta  2019  16500    Automatic     1482   Petrol  145  48.7         1.0\n",
      "     model  year  price transmission  mileage fuelType  tax   mpg  engineSize\n",
      "0      I20  2017   7999       Manual    17307   Petrol  145  58.9         1.2\n",
      "1   Tucson  2016  14499    Automatic    25233   Diesel  235  43.5         2.0\n",
      "2   Tucson  2016  11399       Manual    37877   Diesel   30  61.7         1.7\n",
      "3      I10  2016   6499       Manual    23789   Petrol   20  60.1         1.0\n",
      "4     IX35  2015  10199       Manual    33177   Diesel  160  51.4         2.0\n",
      "       model  year  price transmission  mileage fuelType  tax   mpg  \\\n",
      "0        SLK  2005   5200    Automatic    63000   Petrol  325  32.1   \n",
      "1    S Class  2017  34948    Automatic    27000   Hybrid   20  61.4   \n",
      "2   SL CLASS  2016  49948    Automatic     6200   Petrol  555  28.0   \n",
      "3    G Class  2016  61948    Automatic    16000   Petrol  325  30.4   \n",
      "4    G Class  2016  73948    Automatic     4000   Petrol  325  30.1   \n",
      "\n",
      "   engineSize  \n",
      "0         1.8  \n",
      "1         2.1  \n",
      "2         5.5  \n",
      "3         4.0  \n",
      "4         4.0  \n",
      "   model  year  price transmission  mileage fuelType  tax   mpg  engineSize\n",
      "0   GT86  2016  16000       Manual    24089   Petrol  265  36.2         2.0\n",
      "1   GT86  2017  15995       Manual    18615   Petrol  145  36.2         2.0\n",
      "2   GT86  2015  13998       Manual    27469   Petrol  265  36.2         2.0\n",
      "3   GT86  2017  18998       Manual    14736   Petrol  150  36.2         2.0\n",
      "4   GT86  2017  17498       Manual    36284   Petrol  145  36.2         2.0\n"
     ]
    }
   ],
   "source": [
    "# peek at cars 1 datasets\n",
    "df_audi = pd.read_csv(cars_1_files[0])\n",
    "print(df_audi.head())\n",
    "\n",
    "df_bmw = pd.read_csv(cars_1_files[1])\n",
    "print(df_bmw.head())\n",
    "\n",
    "df_ford = pd.read_csv(cars_1_files[2])\n",
    "print(df_ford.head())\n",
    "\n",
    "df_hyundi = pd.read_csv(cars_1_files[3])\n",
    "print(df_hyundi.head())\n",
    "\n",
    "df_merc = pd.read_csv(cars_1_files[4])\n",
    "print(df_merc.head())\n",
    "\n",
    "df_toyota = pd.read_csv(cars_1_files[5])\n",
    "print(df_toyota.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add column for brand of car\n",
    "df_audi['brand']   = 'audi'\n",
    "df_bmw['brand']    = 'bmw'\n",
    "df_ford['brand']   = 'ford'\n",
    "df_hyundi['brand'] = 'hyundi'\n",
    "df_merc['brand']   = 'merc'\n",
    "df_toyota['brand'] = 'toyota'\n",
    "\n",
    "# concatenate all dataframes together\n",
    "df_cars_1 = pd.concat([df_audi, df_bmw, df_ford, df_hyundi, df_merc, df_toyota])\n",
    "\n",
    "# change column order to something that allows us to split it easier later on\n",
    "df_cars_1 = df_cars_1[['brand', 'model', 'transmission', 'fuelType','year', 'mileage', 'tax', 'mpg', 'engineSize', 'price']]\n",
    "cars_1_y = df_cars_1.pop('price').to_numpy()\n",
    "cars_1_X = df_cars_1.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data\n",
    "Using OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64131, 154)\n",
      "[[  1.    0.    0.  ... 150.   55.4   1.4]\n",
      " [  1.    0.    0.  ...  20.   64.2   2. ]\n",
      " [  1.    0.    0.  ...  30.   55.4   1.4]\n",
      " ...\n",
      " [  1.    0.    0.  ...  20.   70.6   2. ]\n",
      " [  1.    0.    0.  ...  20.   60.1   1.4]\n",
      " [  1.    0.    0.  ...  30.   55.4   1.4]]\n"
     ]
    }
   ],
   "source": [
    "# temporarily separate categorical cols from numerical\n",
    "num_cols = cars_1_X[:,4:]\n",
    "cat_cols = cars_1_X[:, :4]\n",
    "\n",
    "# One-Hot Encode string values\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "cat_cols_enc = enc.fit_transform(cat_cols)\n",
    "\n",
    "cars_1_X_enc = np.hstack((cat_cols_enc, num_cols)).astype(np.float32)\n",
    "print(cars_1_X_enc.shape)\n",
    "print(cars_1_X_enc[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (51304, 154)\n",
      "y_train shape: (51304,)\n",
      "X_test shape: (12827, 154)\n",
      "X_test shape: (12827,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cars_1_X_enc, cars_1_y, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"X_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "## 1. MAE: \n",
    "- It is not very sensitive to outliers in comparison to MSE since it doesn't punish huge errors. \n",
    "- It is usually used when the performance is measured on continuous variable data. \n",
    "- It gives a linear value, which averages the weighted individual differences equally. \n",
    "- The lower the value, better is the model's performance.\n",
    "\n",
    "## 2. MSE: \n",
    "- It is one of the most commonly used metrics, but least useful when a single bad prediction \n",
    "- would ruin the entire model's predicting abilities, i.e when the dataset contains a lot of noise. \n",
    "- It is most useful when the dataset contains outliers, or unexpected values(too high or too low values).\n",
    "\n",
    "## 3. RMSE: \n",
    "- In RMSE, the errors are squared before they are averaged.\n",
    "- This basically implies that RMSE assigns a higher weight to larger errors. \n",
    "- This indicates that RMSE is much more useful when large errors are present and they drastically affect the model's performance. \n",
    "- It avoids taking the absolute value of the error and this trait is useful in many mathematical calculations.\n",
    "- In this metric also, the lower the value, better is the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# TODO - try other loss functions and determine best to use\n",
    "\n",
    "\n",
    "def BaselineModel(optimizer='adam', loss='mean_squared_error',\n",
    "                  activation='relu', output_activation='linear',\n",
    "                  kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                  input_neurons=150, hidden_neurons=200, num_hidden_layers=2):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define INPUT layer\n",
    "    model.add(Dense(input_neurons, input_dim=X_train.shape[1],\n",
    "                    activation=activation,\n",
    "                    kernel_initializer=kernel_initializer, \n",
    "                    bias_initializer=bias_initializer,\n",
    "                    name='layer_input'))\n",
    "    \n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(hidden_neurons, activation=activation,\n",
    "                        kernel_initializer=kernel_initializer, \n",
    "                        bias_initializer=bias_initializer,\n",
    "                        name=f'layer_hidden_{i}'))\n",
    "    \n",
    "    # Define OUTPUT layer\n",
    "    model.add(Dense(1, activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer,\n",
    "                    bias_initializer=bias_initializer, \n",
    "                    name='layer_output'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    # Print out model summary\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define checkpoint callback for model saving\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5'\n",
    "log_dir = 'tensorboard-logs' \n",
    "\n",
    "checkpoint = ModelCheckpoint(f'models/{checkpoint_name}', monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping, tensorboard]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "# Tuning hyper-parameters for precision with 5-fold cross-validation\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Run with default parameters\n",
    "\n",
    "coarse_grid = [\n",
    "    {\n",
    "        'batch_size': [10, 50, 100, 200, 400],\n",
    "        'epochs': [25, 50, 100],\n",
    "        'optimizer': ['Adam'],\n",
    "        'activation': ['relu'],\n",
    "        'num_hidden_layers': [1, 2, 5],\n",
    "        'hidden_neurons': [50, 100, 200, 500]\n",
    "    }\n",
    "]\n",
    "\n",
    "cross_val = 5\n",
    "scores = ['precision', 'recall']\n",
    "result = {}\n",
    "for score in scores:\n",
    "    print('-' * 50)\n",
    "    print(\n",
    "        f\"# Tuning hyper-parameters for {score} with {cross_val}-fold cross-validation\")\n",
    "    print()\n",
    "\n",
    "    # Employ GridSearch using the cross_val variable on the param grid provided\n",
    "    clf = GridSearchCV(\n",
    "        KerasRegressor(build_fn=BaselineModel, batch_size=100, epochs=100, verbose=2),\n",
    "        coarse_grid,\n",
    "        scoring='%s_macro' % score,\n",
    "        # cv=cross_val,\n",
    "        n_jobs=1,\n",
    "        verbose=2\n",
    "    )\n",
    "    # Fit the model on the training labels and outputs\n",
    "    \n",
    "    clf.fit(X_train, y_train,\n",
    "            shuffle=True)\n",
    "    \n",
    "    # clf.fit(X_train, y_train,\n",
    "    #         callbacks=[early_stopping],\n",
    "    #         verbose=2, shuffle=True,\n",
    "    #         workers=2, use_multiprocessing=True)\n",
    "    \n",
    "\n",
    "    print(\"# Best parameters set found on development set:\")\n",
    "    print(f'\\t{clf.best_params_}')\n",
    "    print()\n",
    "    result[score] = clf.best_params_\n",
    "    print(\"# Grid scores on development set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"\\t - %0.3f (+/-%0.03f) for %r\"\n",
    "                % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"# Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35336/45763990.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# history = model_0.fit(X_train, y_train, validation_split=0.2,\n",
    "#                       batch_size=50, epochs=100,\n",
    "#                         callbacks=callbacks_list,\n",
    "#                         verbose=1, shuffle=True,\n",
    "#                         workers=8, use_multiprocessing=True)\n",
    "\n",
    "# Print the precision and recall of the model.\n",
    "# y_pred = model_0.predict(X_test)\n",
    "\n",
    "# print(\"MAE\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "# print(history[1].params)\n",
    "\n",
    "# loss_train = history['train_loss']\n",
    "# loss_val = history['val_loss']\n",
    "# epochs = range(1, 100)\n",
    "# plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "# plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "# plt.title('Training and Validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# loss_train = history['acc']\n",
    "# loss_val = history['val_acc']\n",
    "# epochs = range(1, 100)\n",
    "# plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
    "# plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
    "# plt.title('Training and Validation accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
